"""
Loader module for importing Rust-generated content.db into NetworkX for R&D analysis.

This is the bridge between the fast Rust generator (iqrah-gen) and Python R&D tools.
"""

import sqlite3
from pathlib import Path
from typing import Optional

import networkx as nx
from loguru import logger


def load_graph_from_content_db(
    db_path: str | Path,
    include_content_nodes: bool = True,
    include_knowledge_nodes: bool = True,
) -> nx.DiGraph:
    """
    Load the knowledge graph from a Rust-generated content.db file.

    Args:
        db_path: Path to content.db file generated by iqrah-gen
        include_content_nodes: Include Chapter, Verse, Word, Root, Lemma nodes
        include_knowledge_nodes: Include Knowledge axis nodes

    Returns:
        NetworkX DiGraph with all nodes and edges

    Example:
        >>> G = load_graph_from_content_db("content.db")
        >>> print(f"Graph has {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
    """
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row

    G = nx.DiGraph()

    # Node type mapping (matches iqrah-core enum)
    NODE_TYPES = {
        1: "chapter",
        2: "verse",
        3: "word",
        4: "word_instance",
        5: "knowledge",
        6: "root",
        7: "lemma",
    }

    # Edge type mapping
    EDGE_TYPES = {
        0: "dependency",
        1: "knowledge",
    }

    # Load nodes
    logger.info("Loading nodes...")
    node_filter_types = []
    if include_content_nodes:
        node_filter_types.extend([1, 2, 3, 4, 6, 7])  # All content types
    if include_knowledge_nodes:
        node_filter_types.append(5)  # Knowledge

    type_placeholders = ",".join("?" * len(node_filter_types))
    cursor = conn.execute(
        f"SELECT id, ukey, node_type FROM nodes WHERE node_type IN ({type_placeholders})",
        node_filter_types,
    )

    node_count = 0
    for row in cursor:
        G.add_node(
            row["ukey"],
            id=row["id"],
            node_type=NODE_TYPES.get(row["node_type"], "unknown"),
        )
        node_count += 1

    logger.info(f"Loaded {node_count} nodes")

    # Load edges (with source/target ukeys via JOIN)
    logger.info("Loading edges...")
    cursor = conn.execute("""
        SELECT
            n1.ukey AS source_ukey,
            n2.ukey AS target_ukey,
            e.edge_type,
            e.distribution_type,
            e.param1 AS weight,
            e.param2
        FROM edges e
        JOIN nodes n1 ON e.source_id = n1.id
        JOIN nodes n2 ON e.target_id = n2.id
    """)

    edge_count = 0
    for row in cursor:
        # Only add edge if both nodes are in graph
        if row["source_ukey"] in G and row["target_ukey"] in G:
            G.add_edge(
                row["source_ukey"],
                row["target_ukey"],
                edge_type=EDGE_TYPES.get(row["edge_type"], "unknown"),
                weight=row["weight"],
            )
            edge_count += 1

    logger.info(f"Loaded {edge_count} edges")

    conn.close()
    return G


def load_node_metadata(G: nx.DiGraph, db_path: str | Path) -> None:
    """
    Load node metadata (scores, etc.) and attach to graph nodes.

    Args:
        G: NetworkX graph (modified in place)
        db_path: Path to content.db
    """
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row

    cursor = conn.execute("""
        SELECT n.ukey, m.key, m.value
        FROM node_metadata m
        JOIN nodes n ON m.node_id = n.id
    """)

    count = 0
    for row in cursor:
        ukey = row["ukey"]
        if ukey in G:
            G.nodes[ukey][row["key"]] = row["value"]
            count += 1

    logger.info(f"Loaded {count} metadata entries")
    conn.close()


def write_experimental_edges(
    db_path: str | Path,
    edges: list[tuple[str, str, str, float]],
    experiment_name: str,
) -> int:
    """
    Write experimental edges to content.db for R&D experiments.

    This allows Python R&D notebooks to propose new edge types that Rust can read back.

    Args:
        db_path: Path to content.db
        edges: List of (source_ukey, target_ukey, edge_type, weight) tuples
        experiment_name: Name of the experiment (e.g., "semantic_similarity_v1")

    Returns:
        Number of edges written

    Example:
        >>> edges = [
        ...     ("VERSE:1:1", "VERSE:2:1", "semantic_similarity", 0.85),
        ...     ("VERSE:1:2", "VERSE:2:2", "semantic_similarity", 0.72),
        ... ]
        >>> write_experimental_edges("content.db", edges, "semantic_similarity_v1")
        2
    """
    conn = sqlite3.connect(db_path)

    # Ensure table exists (in case schema hasn't been migrated)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS experimental_edges (
            source_ukey TEXT NOT NULL,
            target_ukey TEXT NOT NULL,
            edge_type TEXT NOT NULL,
            weight REAL NOT NULL DEFAULT 1.0,
            experiment_name TEXT NOT NULL,
            created_at INTEGER NOT NULL DEFAULT (unixepoch()),
            PRIMARY KEY (source_ukey, target_ukey, experiment_name)
        )
    """)

    cursor = conn.executemany(
        """
        INSERT OR REPLACE INTO experimental_edges
        (source_ukey, target_ukey, edge_type, weight, experiment_name)
        VALUES (?, ?, ?, ?, ?)
        """,
        [(s, t, e, w, experiment_name) for s, t, e, w in edges],
    )

    count = cursor.rowcount
    conn.commit()
    conn.close()

    logger.info(f"Wrote {len(edges)} experimental edges for '{experiment_name}'")
    return len(edges)


def load_experimental_edges(
    db_path: str | Path,
    experiment_name: Optional[str] = None,
) -> nx.DiGraph:
    """
    Load experimental edges from content.db into a NetworkX graph.

    Args:
        db_path: Path to content.db
        experiment_name: Optional filter by experiment name

    Returns:
        NetworkX DiGraph with experimental edges
    """
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row

    G = nx.DiGraph()

    query = "SELECT source_ukey, target_ukey, edge_type, weight, experiment_name FROM experimental_edges"
    params = []
    if experiment_name:
        query += " WHERE experiment_name = ?"
        params.append(experiment_name)

    cursor = conn.execute(query, params)

    for row in cursor:
        G.add_edge(
            row["source_ukey"],
            row["target_ukey"],
            edge_type=row["edge_type"],
            weight=row["weight"],
            experiment=row["experiment_name"],
        )

    logger.info(f"Loaded {G.number_of_edges()} experimental edges")
    conn.close()
    return G


def clear_experiment(db_path: str | Path, experiment_name: str) -> int:
    """
    Remove all edges for a specific experiment.

    Args:
        db_path: Path to content.db
        experiment_name: Name of experiment to clear

    Returns:
        Number of edges deleted
    """
    conn = sqlite3.connect(db_path)
    cursor = conn.execute(
        "DELETE FROM experimental_edges WHERE experiment_name = ?",
        (experiment_name,),
    )
    count = cursor.rowcount
    conn.commit()
    conn.close()
    logger.info(f"Cleared {count} edges from experiment '{experiment_name}'")
    return count


if __name__ == "__main__":
    # Example usage
    import sys

    if len(sys.argv) < 2:
        print("Usage: python loader.py <content.db>")
        sys.exit(1)

    db_path = sys.argv[1]
    G = load_graph_from_content_db(db_path)
    print(f"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")

    # Print sample nodes
    print("\nSample nodes:")
    for i, (node, data) in enumerate(G.nodes(data=True)):
        if i >= 5:
            break
        print(f"  {node}: {data}")


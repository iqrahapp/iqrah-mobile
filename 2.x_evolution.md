# ISS Evolution: The 2.x Era
**From "Death Spiral" to Robust Production Stability**

## Executive Summary

The 2.x cycle was defined by a shift from **naive simulation** to **principled cognitive modeling**. We started v2.5 with a broken model that collapsed under load ("Death Spiral"). By v2.9, we have achieved a robust, self-correcting system that beats random-walk baselines by 97% on memorization tasks, verified by a new trials-based evaluation framework.

| Metric | v2.5 (The "Death Spiral") | v2.9.3 (Current State) | Improvement |
|--------|---------------------------|------------------------|-------------|
| **Coverage (Juz 30)** | 0.2% | 65.6% | **300x** |
| **Failure Rate** | 67% | < 7% | **-90%** |
| **Give-ups** | 100% | 0% | Fixed |
| **Evaluation Model** | Binary (Yes/No) | Trials-Based (Cognitive) | High Fidelity |

---

## 1. The Crisis: The "Death Spiral" (v2.5)

In v2.5, we attempted to simulate large goals (Juz 30) using simple spaced repetition. The result was catastrophic:
*   **Problem**: Greedy introduction flooded the user with 549 items in 30 days.
*   **Result**: The exponential decay curve was too harsh for young items. As items failed, their stability dropped, rescheduling them sooner, creating an infinite review loop.
*   **Outcome**: 67% failure rate, 0.2% coverage. The system was unusable.

---

## 2. The Turnaround: Fixing the Physics (v2.6 - v2.7)

We solved this in two stages: **Architecture** and **Architecture**.

### v2.6: Architecture Fix (Cluster Gating)
We introduced "Blocked Practice" constraints to stop the flood:
*   **Cluster Gate**: New items are only introduced if the current "active cluster" (e.g., current surah) is sufficiently stable (Energy > 0.4).
*   **Working Set Limits**: Cap on total active items (e.g., 50).
*   *Result*: Stabilized the item count, but items still weren't maturing.

### v2.7: Algorithm Fix (FSRS Power Law)
**The Breakthrough**: We replaced the aggressive exponential forgetting curve with the FSRS power law.
*   **Old**: `R = exp(-t/S)` (Harsh drop-off)
*   **New**: `R = (1 + t/9S)^-1` (Long tail retention)
*   **Impact**: Young items survive 6x longer before failing. This single change allowed items to mature, boosting coverage from **1.4% to 42%**.

---

## 3. The Proof: Realistic Evaluation (v2.8)

With the simulation stable, we realized our evaluation metric was wrong. We were measuring "Binary Recall" (Did you get it right?), which ignored the *quality* of recall.

We built the **Exercise Framework (v2.8)** to enable axis-aware cognitive evaluation:
*   **Trials-Based Modeling**: Instead of "Pass/Fail", we model "Attempts until success" using a Geometric Distribution.
*   **The Result**:
    *   **Memorization (Sequential)**: `iqrah` (Cluster) scores **1.0**, while `random` scores **0.0**. Random practice creates scattered knowledge, leading to massive hesitation (6+ trials/word).
    *   **Vocabulary (Independent)**: `iqrah` and `random` score evenly (~0.60), proving our model correctly identifies when distributed practice is valid.

This proved that our Cluster Strategy is objectively superior for *Hifz* (memorization).

---

## 4. The Hardening: Production Readiness (v2.9)

With the core confirmed, v2.9 focused on robustness, fairness, and safety.

### M2.4: Principled Introduction Policy
A 7-stage pipeline now decides when to introduce items, balancing **Capacity** (can we afford it?) vs **Growth** (do we need it?):
*   **Floor**: Ensures we never completely stall (`intro_min_per_day`).
*   **Ceiling**: Hard stops when `max_working_set` is full.

### M2.6: Backlog-Awareness
Preventing future death spirals:
*   **Dynamic Floor**: If `p90_due_age` gets too high (severe backlog), the system automatically stops introducing new items until the backlog is cleared.

### M2.7: The Fairness Fix
We discovered "Overdue Starvation" where new items were starving out old, overdue items.
*   **Fix**: Strict sorting by `due_age DESC`. Oldest problems are *always* solved first.

### M2.9: Robust Verification
*   **Metric Integrity**: Split "Horizon" (theoretical max) vs "Today" (actual state) metrics.
*   **Multi-Seed Validation**: All benchmarks now run `n=30` seeds to ensure results aren't flukes.

---

## 5. Latest Results & Trace Analysis (v2.9.3)

Recent large-scale validation (n=30 seeds, 180 days) confirms the system is now production-ready.

### The Numbers (Juz 30 Dedicated)
Using the optimal `session_size=40` configuration:

| Metric | Value | Threshold | Status |
|--------|-------|-----------|--------|
| **Coverage** | **65.6%** | ≥ 63% | ✅ PASS |
| **At-Risk Items (R < 0.9)** | **0.95%** | ≤ 6% | ✅ PASS (Exceeds Safety) |
| **Tail Health (p10 R)** | **0.928** | ≥ 0.88 | ✅ PASS |
| **Retention (mean R)** | **0.959** | - | Excellent |
| **Give-ups** | **0%** | 0% | Fixed |

### Trace Dynamics: How It Works
Trace analysis reveals the "Pulse" of the v2.9 engine:

1.  **Bootstrap Phase (Days 0-10)**:
    *   **Behavior**: The "Bootstrap" logic bypasses the stability gate.
    *   **Result**: Rapid introduction of ~100 items to fill the initial working set buffer.

2.  **Stabilization Phase (Days 11-40)**:
    *   **Behavior**: Gate activates (`cluster_energy < 0.4`). Introductions slow down to `intro_min_per_day` (e.g., 5).
    *   **Result**: The system focuses on maturing the initial 100 items while slowly introducing new ones (Soft Stop).

3.  **Expansion Phase (Day 40+)**:
    *   **Behavior**: Older items mature, `cluster_energy` rises.
    *   **Result**: The gate opens, allowing batches of new items to flow in until the `max_working_set` or review budget (0.06 reviews/item) is hit.

4.  **Backlog Safety Trigger (Rare)**:
    *   **Condition**: If `p90_due_age > 45 days` (user falls behind).
    *   **Action**: Trace shows `intro_floor` drops to 0.
    *   **Result**: 100% of capacity is diverted to clearing the backlog. No new items until health is restored.

---

## 6. Cognitive Validation: The Exercise Framework

The most significant validation of the 2.x architecture came from the **v2.8 Exercise Framework**, which allowed us to test *how* users learn, not just *what* they reviewed.

### 6.1. The Recall Quality Gap
We verified that while `Random` schedulers can show high "Coverage" (items seen once), they fail to build **sequential fluency** required for Hifz.

| Exercise Type | Metric | `iqrah` (Cluster) | `random` (Baseline) | Win |
|---------------|--------|-------------------|---------------------|-----|
| **Memorization** | **Score (0-1)** | **1.0 (Perfect)** | **0.0 (Fail)** | **IQRAH** |
| (Sequential) | Avg Trials/Word | **1.47** (Smooth) | **6.46** (Struggle) | **-77%** |
| | Grade | **Easy** | **Again** | |
| | Longest Streak | Full Surah | Broken | |
| **Vocabulary** | **Score (0-1)** | 0.59 | **0.61** | **TIE** |
| (Independent) | Accuracy | ~60% | ~62% | |

**Conclusion**:
*   **Hifz (Memorization)** behaves like a linked list. `Random` breaks the links, causing "stuttering" recall (high trials). `Iqrah` enforces blocked practice, creating strong sequential chains.
*   **Vocabulary** behaves like a hash map. Both strategies work equally well. This confirms our simulation correctly distinguishes between learning types.

### 6.2. The Saturation Problem & v2.9 Fix
In early v2.9, we found that exercise scores on large goals (Juz 30) were artificially low ("Again") even for strong profiles.
*   **Cause**: We were averaging unintroduced items (trial count = 20) with learned items.
*   **Fix**: Separated **Availability** (Coverage) from **Recall Quality** (Mastery).

**New Report Format (v2.9)**:
> **Recitation**: 166 attempted (29.6% availability)
> *   **Attempted**: avg 2.45 trials/item, Grade: **Good**
> *   **Unavailable**: 398 items (not yet introduced)

This granularity allows us to see that the user has **mastered** the 30% they have learned, rather than labeling them a failure because they haven't finished the book yet.

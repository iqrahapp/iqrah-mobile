# Sprint 7: Research Pipeline Refactoring

**Date:** 2025-10-04
**Purpose:** Transform the R&D knowledge graph generation from Jupyter notebooks to production-ready Python package

---

## Current State of R&D Pipeline

### Directory Structure (Draft/POC)
```
research_and_dev/iqrah-knowledge-graph/
├── data/
│   └── quran-morphology-v0.5.csv          # Input data
├── notebooks/
│   └── knowledge_experiments.ipynb         # Notebook does EVERYTHING
├── scripts/
│   └── [various Python scripts]            # Ad-hoc, not organized
└── .cache/                                 # Cached web API responses (OFFLINE!)
```

### Current Workflow Problems ❌

**Step 1: Python Script** (generates base graph from morphology CSV)
- Ad-hoc, not testable
- Hardcoded paths
- No configuration
- No error handling

**Step 2: Jupyter Notebook** (adds metadata, PageRank, exports CBOR)
- **CRITICAL ISSUE:** Fetches metadata from web API (now offline!)
- Only works because of `.cache/` directory
- Monolithic notebook (thousands of lines)
- Cannot be automated
- Cannot be version controlled properly
- No reproducibility guarantees
- **Hardcodes metadata INTO the knowledge graph** (wrong separation!)

**Output:** `iqrah-graph-v1.0.1.cbor.zst`
- Contains EVERYTHING mixed together
- Content + metadata bundled
- Cannot update translations without rebuilding entire graph
- No schema validation

---

## Root Cause Analysis

### Why This Worked for MVP
1. ✅ Fast prototyping
2. ✅ Exploratory data analysis
3. ✅ Proof of concept

### Why This Won't Scale
1. ❌ **Offline Web API:** Cannot regenerate graph (dependent on cache)
2. ❌ **No Separation:** Metadata mixed with graph structure
3. ❌ **Not Reproducible:** Notebook state-dependent
4. ❌ **Hard to Change:** Any graph change requires manual notebook edits
5. ❌ **No Testing:** Cannot validate graph correctness
6. ❌ **No Automation:** Cannot integrate into CI/CD

---

## Sprint 7 R&D Goals

### Phase 1: Migrate to Proper Python Package Structure

**New Structure:**
```
research_and_dev/iqrah-knowledge-graph/
├── pyproject.toml                    # Modern Python packaging
├── README.md                          # Usage documentation
├── iqrah_kg/                          # Main package
│   ├── __init__.py
│   ├── cli.py                         # CLI entry point (Click/Typer)
│   ├── config.py                      # Configuration management
│   ├── models/                        # Data models (Pydantic)
│   │   ├── __init__.py
│   │   ├── node.py
│   │   ├── edge.py
│   │   └── graph.py
│   ├── loaders/                       # Data loading
│   │   ├── __init__.py
│   │   ├── morphology.py              # CSV → Graph
│   │   ├── tarteel_db.py              # SQLite metadata
│   │   └── quran_text.py              # Text loading
│   ├── builders/                      # Graph construction
│   │   ├── __init__.py
│   │   ├── base_graph.py              # Morphology-based graph
│   │   ├── knowledge_axes.py          # Add learning axes
│   │   └── relationships.py           # Edge creation
│   ├── metrics/                       # PageRank, importance scores
│   │   ├── __init__.py
│   │   ├── pagerank.py
│   │   └── foundational.py
│   ├── exporters/                     # Output formats
│   │   ├── __init__.py
│   │   ├── cbor.py                    # Knowledge graph only
│   │   ├── content_db.py              # SQLite content.db
│   │   └── validator.py               # Schema validation
│   └── utils/
│       ├── __init__.py
│       └── logging.py
├── tests/                             # Pytest tests
│   ├── conftest.py
│   ├── test_loaders.py
│   ├── test_builders.py
│   └── test_metrics.py
├── data/
│   ├── input/
│   │   ├── quran-morphology-v0.5.csv
│   │   └── tarteel_metadata.db        # NEW: SQLite from Tarteel
│   └── output/
│       ├── knowledge_graph.cbor.zst   # PURE graph (no metadata)
│       └── content.db                  # Metadata in SQLite
└── configs/
    ├── default.yaml
    └── production.yaml
```

### Phase 2: Separate Concerns (Critical!)

**OLD Approach (Everything in CBOR):**
```
iqrah-graph-v1.0.1.cbor.zst
├── nodes
│   ├── id
│   ├── type
│   └── attributes
│       ├── arabic          ❌ Metadata
│       ├── translation     ❌ Metadata
│       ├── audio_url       ❌ Metadata
│       ├── foundational_score ✅ Graph metric
│       └── influence_score    ✅ Graph metric
└── edges
```

**NEW Approach (Separation):**

**1. knowledge_graph.cbor.zst** (Pure graph structure)
```
Graph ONLY contains:
- Nodes (id, type, created_at)
- Edges (source, target, type, distribution params)
- Importance scores (foundational_score, influence_score)
- Linguistic structure (root, lemma relationships)
```

**2. content.db** (SQLite with metadata)
```sql
-- Generated by R&D pipeline
-- Consumed by Rust app

-- Quran text
CREATE TABLE quran_text (...);

-- Translations (from Tarteel DB)
CREATE TABLE translations (...);

-- Audio resources (from Tarteel DB)
CREATE TABLE audio_resources (...);

-- Reciters metadata
CREATE TABLE reciters (...);

-- Surahs
CREATE TABLE surahs (...);
```

**Benefits:**
- ✅ Can update translations without touching graph
- ✅ Can add new languages easily
- ✅ Graph represents pure knowledge structure
- ✅ Metadata managed separately

### Phase 3: Replace Web API with Tarteel SQLite

**Current (Broken):**
```python
# In notebook
response = requests.get(f"https://api.quran.com/v4/...")  # OFFLINE!
# Relies on .cache/ to work
```

**New Approach:**
```python
# iqrah_kg/loaders/tarteel_db.py
import sqlite3
from pathlib import Path

class TarteelMetadataLoader:
    def __init__(self, db_path: Path):
        self.conn = sqlite3.connect(db_path)

    def get_translations(self, node_id: str, languages: list[str]) -> dict:
        """Load translations from Tarteel SQLite DB."""
        query = """
            SELECT language_code, translation, translator
            FROM translations
            WHERE node_id = ? AND language_code IN (?, ?, ...)
        """
        # ... implementation

    def get_audio_urls(self, node_id: str, reciters: list[str]) -> dict:
        """Load audio URLs from Tarteel SQLite DB."""
        # ... implementation
```

**Setup:**
1. Download Tarteel SQLite databases (free, offline)
2. Store in `data/input/tarteel_metadata.db`
3. Load directly (no web API dependency)
4. Version control the DB schema (not the data)

---

## Implementation Plan

### Week 1: Python Package Structure

**Day 1: Project Setup**
```bash
cd research_and_dev/iqrah-knowledge-graph

# Create modern Python project
cat > pyproject.toml <<EOF
[build-system]
requires = ["setuptools>=68.0"]
build-backend = "setuptools.build_meta"

[project]
name = "iqrah-kg"
version = "2.0.0"
description = "Iqrah Knowledge Graph Generator"
requires-python = ">=3.11"
dependencies = [
    "pandas>=2.0",
    "networkx>=3.0",
    "pydantic>=2.0",
    "click>=8.0",
    "cbor2>=5.0",
    "zstandard>=0.21",
    "pyyaml>=6.0",
    "sqlalchemy>=2.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "pytest-cov>=4.0",
    "black>=23.0",
    "ruff>=0.1.0",
    "mypy>=1.0",
]

[project.scripts]
iqrah-kg = "iqrah_kg.cli:main"
EOF

# Install in development mode
pip install -e ".[dev]"
```

**Day 2-3: Extract Notebook Logic**

Convert notebook cells to proper modules:

1. **Morphology Parsing** → `iqrah_kg/loaders/morphology.py`
```python
from pydantic import BaseModel
import pandas as pd

class MorphologyNode(BaseModel):
    id: str
    node_type: str
    root: str | None
    lemma: str | None
    # ... other fields

def load_morphology_csv(path: Path) -> list[MorphologyNode]:
    """Parse quran-morphology-v0.5.csv into validated nodes."""
    df = pd.read_csv(path)
    nodes = []
    for _, row in df.iterrows():
        node = MorphologyNode(
            id=f"WORD_INSTANCE:{row['chapter']}:{row['verse']}:{row['word']}",
            node_type="word_instance",
            root=row.get('root'),
            lemma=row.get('lemma'),
        )
        nodes.append(node)
    return nodes
```

2. **Graph Building** → `iqrah_kg/builders/base_graph.py`
```python
import networkx as nx

class GraphBuilder:
    def __init__(self):
        self.graph = nx.DiGraph()

    def add_morphology_nodes(self, nodes: list[MorphologyNode]):
        """Add nodes from morphology data."""
        for node in nodes:
            self.graph.add_node(node.id, **node.model_dump())

    def add_dependency_edges(self):
        """Create edges based on linguistic dependencies."""
        # ... extracted from notebook

    def add_knowledge_edges(self):
        """Create knowledge propagation edges."""
        # ... extracted from notebook

    def build(self) -> nx.DiGraph:
        """Build complete graph."""
        return self.graph
```

3. **PageRank Calculation** → `iqrah_kg/metrics/pagerank.py`
```python
def calculate_influence_scores(graph: nx.DiGraph) -> dict[str, float]:
    """Calculate PageRank-based influence scores."""
    pagerank = nx.pagerank(graph, alpha=0.85)
    return pagerank

def calculate_foundational_scores(graph: nx.DiGraph) -> dict[str, float]:
    """Calculate prerequisite-based foundational scores."""
    # ... extracted from notebook
    return scores
```

### Week 2: Metadata Separation

**Day 1: Download Tarteel DBs**
```bash
# Download from Tarteel (free)
wget https://tarteel.ai/datasets/quran_metadata.db
mv quran_metadata.db data/input/tarteel_metadata.db

# Inspect schema
sqlite3 data/input/tarteel_metadata.db ".schema"
```

**Day 2-3: Metadata Loader**
```python
# iqrah_kg/loaders/tarteel_db.py
class TarteelMetadataLoader:
    def load_all_translations(self, language: str = "en") -> pd.DataFrame:
        """Load all translations for a language."""
        query = "SELECT * FROM translations WHERE language_code = ?"
        return pd.read_sql(query, self.conn, params=[language])

    def load_audio_resources(self, reciter: str) -> pd.DataFrame:
        """Load audio URLs for a reciter."""
        query = "SELECT * FROM audio WHERE reciter_id = ?"
        return pd.read_sql(query, self.conn, params=[reciter])
```

**Day 4: Content DB Exporter**
```python
# iqrah_kg/exporters/content_db.py
import sqlite3
from pathlib import Path

def export_to_content_db(
    graph: nx.DiGraph,
    metadata_loader: TarteelMetadataLoader,
    output_path: Path
):
    """Export graph + metadata to content.db (SQLite)."""
    conn = sqlite3.connect(output_path)

    # 1. Create schema (from 02-DATABASE-SCHEMA-DESIGN.md)
    conn.executescript(CONTENT_DB_SCHEMA)

    # 2. Insert nodes
    for node_id, attrs in graph.nodes(data=True):
        conn.execute(
            "INSERT INTO nodes (id, node_type, created_at) VALUES (?, ?, ?)",
            (node_id, attrs['node_type'], int(time.time()))
        )

    # 3. Insert edges
    for source, target, attrs in graph.edges(data=True):
        conn.execute(
            "INSERT INTO edges (...) VALUES (...)",
            # ... params
        )

    # 4. Insert metadata (from Tarteel)
    translations = metadata_loader.load_all_translations("en")
    translations.to_sql("translations", conn, if_exists="append", index=False)

    # 5. Insert importance scores
    influence = calculate_influence_scores(graph)
    foundational = calculate_foundational_scores(graph)
    # ... insert to importance_scores table

    conn.commit()
    conn.close()
```

### Week 3: CLI & Automation

**Day 1-2: CLI Tool**
```python
# iqrah_kg/cli.py
import click

@click.group()
def cli():
    """Iqrah Knowledge Graph Generator."""
    pass

@cli.command()
@click.option('--morphology', type=click.Path(exists=True), required=True)
@click.option('--metadata-db', type=click.Path(exists=True), required=True)
@click.option('--output-graph', type=click.Path(), default='output/knowledge_graph.cbor.zst')
@click.option('--output-db', type=click.Path(), default='output/content.db')
def build(morphology, metadata_db, output_graph, output_db):
    """Build knowledge graph and content database."""
    click.echo("Loading morphology data...")
    nodes = load_morphology_csv(Path(morphology))

    click.echo("Building graph...")
    builder = GraphBuilder()
    builder.add_morphology_nodes(nodes)
    builder.add_dependency_edges()
    builder.add_knowledge_edges()
    graph = builder.build()

    click.echo("Calculating metrics...")
    influence = calculate_influence_scores(graph)
    foundational = calculate_foundational_scores(graph)

    click.echo("Exporting knowledge graph...")
    export_to_cbor(graph, Path(output_graph))

    click.echo("Exporting content database...")
    metadata_loader = TarteelMetadataLoader(Path(metadata_db))
    export_to_content_db(graph, metadata_loader, Path(output_db))

    click.echo("✅ Build complete!")

@cli.command()
@click.option('--graph', type=click.Path(exists=True), required=True)
def validate(graph):
    """Validate knowledge graph schema."""
    # ... validation logic

@cli.command()
@click.option('--graph', type=click.Path(exists=True), required=True)
def stats(graph):
    """Show graph statistics."""
    g = load_cbor(Path(graph))
    click.echo(f"Nodes: {g.number_of_nodes()}")
    click.echo(f"Edges: {g.number_of_edges()}")
    # ... more stats

if __name__ == '__main__':
    cli()
```

**Day 3: Testing**
```python
# tests/test_builders.py
import pytest

def test_morphology_loader():
    nodes = load_morphology_csv(Path("data/input/quran-morphology-v0.5.csv"))
    assert len(nodes) > 0
    assert all(isinstance(n, MorphologyNode) for n in nodes)

def test_graph_builder():
    builder = GraphBuilder()
    builder.add_morphology_nodes(sample_nodes)
    builder.add_dependency_edges()
    graph = builder.build()

    assert graph.number_of_nodes() == len(sample_nodes)
    assert graph.number_of_edges() > 0

def test_pagerank_calculation():
    graph = create_test_graph()
    scores = calculate_influence_scores(graph)

    assert all(0 <= score <= 1 for score in scores.values())
    assert sum(scores.values()) > 0
```

**Day 4-5: Documentation & CI**

Create `README.md`:
```markdown
# Iqrah Knowledge Graph Generator

## Installation
```bash
pip install -e ".[dev]"
```

## Usage

### Build Knowledge Graph
```bash
iqrah-kg build \
    --morphology data/input/quran-morphology-v0.5.csv \
    --metadata-db data/input/tarteel_metadata.db \
    --output-graph output/knowledge_graph.cbor.zst \
    --output-db output/content.db
```

### Validate Graph
```bash
iqrah-kg validate --graph output/knowledge_graph.cbor.zst
```

### Show Statistics
```bash
iqrah-kg stats --graph output/knowledge_graph.cbor.zst
```

## Output Files

1. `knowledge_graph.cbor.zst` - Pure graph structure (nodes, edges, metrics)
2. `content.db` - SQLite database with metadata (translations, audio, etc.)

## Testing
```bash
pytest tests/ -v --cov=iqrah_kg
```
```

**GitHub Actions CI:**
```yaml
# .github/workflows/kg-pipeline.yml
name: Knowledge Graph Pipeline

on:
  push:
    paths:
      - 'research_and_dev/iqrah-knowledge-graph/**'

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          cd research_and_dev/iqrah-knowledge-graph
          pip install -e ".[dev]"

      - name: Run tests
        run: |
          cd research_and_dev/iqrah-knowledge-graph
          pytest tests/ -v --cov=iqrah_kg

      - name: Build graph
        run: |
          cd research_and_dev/iqrah-knowledge-graph
          iqrah-kg build \
            --morphology data/input/quran-morphology-v0.5.csv \
            --metadata-db data/input/tarteel_metadata.db

      - name: Validate output
        run: |
          cd research_and_dev/iqrah-knowledge-graph
          iqrah-kg validate --graph output/knowledge_graph.cbor.zst
```

---

## Integration with Rust App

### New Import Flow

**Before (Sprint 1-6):**
```
notebook → iqrah-graph-v1.0.1.cbor.zst → Rust import → Single DB
```

**After (Sprint 7+):**
```
Python CLI → {knowledge_graph.cbor.zst, content.db} → Rust uses both
```

**Rust Side Changes:**
```rust
// OLD: Import everything from CBOR
pub async fn import_cbor_graph_from_bytes(data: Vec<u8>) -> Result<()> {
    // Parse CBOR, extract nodes, edges, AND metadata
    // Insert everything into single database
}

// NEW: Two-step process
pub async fn setup_content_db(cbor_path: String, content_db_path: String) -> Result<()> {
    // 1. Import pure graph from CBOR
    let graph = parse_cbor(&cbor_path)?;

    // 2. content.db is ALREADY built by Python CLI
    //    Just copy it to the right location
    std::fs::copy(content_db_path, app_content_db_path)?;

    Ok(())
}
```

---

## Success Criteria

### Python Package ✅
- [ ] Proper `pyproject.toml` structure
- [ ] Type hints throughout (mypy passes)
- [ ] 80%+ test coverage
- [ ] CLI tool works (`iqrah-kg build`)
- [ ] CI/CD pipeline validates builds

### Data Separation ✅
- [ ] `knowledge_graph.cbor.zst` contains ONLY graph structure
- [ ] `content.db` contains ONLY metadata
- [ ] No web API dependencies (uses Tarteel SQLite)
- [ ] Reproducible builds (same input → same output)

### Documentation ✅
- [ ] README with clear usage instructions
- [ ] Architecture diagram (graph vs metadata)
- [ ] API documentation (Sphinx)
- [ ] Contributing guide

### Integration ✅
- [ ] Rust app works with new outputs
- [ ] Flutter app uses separated databases
- [ ] Migration path for existing users

---

## Timeline (Parallel with Rust Refactor)

| Week | Python R&D | Rust/Flutter App |
|------|------------|------------------|
| 1 | Package structure, extract notebook logic | Workspace setup, domain models |
| 2 | Metadata separation, Tarteel DB integration | Storage layer (SQLx) |
| 3 | CLI tool, testing, CI/CD | API update, migration |

**Total: 3 weeks** (same as Rust refactor, done in parallel)

---

## Benefits of This Approach

### Immediate
- ✅ **No more offline API dependency**
- ✅ **Reproducible graph generation**
- ✅ **Testable pipeline**
- ✅ **Version controlled properly**

### Long-term
- ✅ **Easy to modify graph structure** (just update Python code)
- ✅ **Easy to add metadata** (just update content.db schema)
- ✅ **Easy to add languages** (update Tarteel DB queries)
- ✅ **CI/CD can validate graphs** before shipping

### For AI Agents
- ✅ **Can improve Python pipeline** autonomously
- ✅ **Can test changes** before deploying
- ✅ **Can generate new graph versions** automatically
- ✅ **Can experiment with new metrics** safely

---

## Next Steps

1. **Download Tarteel SQLite DBs** (you do this manually)
2. **Review this design** with your R&D experience
3. **Approve approach** or suggest modifications
4. **Execute in parallel** with Rust refactor

This transforms the R&D pipeline from "works on my machine" to "production-ready data engineering."

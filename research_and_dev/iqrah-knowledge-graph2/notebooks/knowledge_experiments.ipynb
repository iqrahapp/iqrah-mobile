{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from loguru import logger\n\n# Use offline data loader instead of API\nfrom iqrah.quran_offline import load_quran_offline\n\nlogger.remove()\n\n# Load Quran data from local JSON files (no API calls)\nprint(\"Loading Quran data from offline sources...\")\nquran = load_quran_offline()\nprint(f\"‚úì Loaded {len(quran.chapters)} chapters, {quran.total_verses()} verses\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quran.chapters[1].verses[1].words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Old method\n",
    "import networkx as nx\n",
    "\n",
    "from iqrah.graph.identifiers import NIG, NIP\n",
    "from iqrah.graph.knowledge import Distribution, KnowledgeEdgeManager\n",
    "from iqrah.graph.node_manager import NodeManager\n",
    "from iqrah.quran_api.models import Chapter, Verse, Word\n",
    "\n",
    "\n",
    "class KnowledgeExperiments:\n",
    "    def __init__(self, graph: nx.DiGraph, quran: Quran):\n",
    "        self.edge_manager = KnowledgeEdgeManager(graph)\n",
    "        self.G = graph\n",
    "        self.quran = quran\n",
    "        self.node_manager = NodeManager(graph)\n",
    "        self._is_compiled = False\n",
    "\n",
    "    def get_nodes_by_type(self, node_type: str) -> set[str]:\n",
    "        return self.node_manager.get_nodes_by_type(node_type)\n",
    "\n",
    "    def get_verse_words(self, verse_id: str) -> list[str]:\n",
    "        return self.node_manager.get_verse_words(verse_id)\n",
    "\n",
    "    def get_chapter_verses(self, chapter_id: str) -> list[str]:\n",
    "        return self.node_manager.get_chapter_verses(chapter_id)\n",
    "\n",
    "    def has_tajweed_rules(self, word_id: str) -> bool:\n",
    "        \"\"\"Check if word has tajweed rules (placeholder)\"\"\"\n",
    "        node = self.G.nodes.get(word_id)\n",
    "        if not node:\n",
    "            return False\n",
    "        return node.get('has_tajweed', False)\n",
    "\n",
    "    def get_word_root(self, word_id: str, cutoff=3) -> str | None:\n",
    "        \"\"\"Get root of a word by traversing the graph\"\"\"\n",
    "        for path in nx.all_simple_paths(self.G, word_id,\n",
    "                                      self.node_manager.get_nodes_by_type(\"root\"), cutoff=cutoff):\n",
    "            return path[-1]\n",
    "        return None\n",
    "\n",
    "    def setup_standard_memorization(self):\n",
    "        \"\"\"\n",
    "        Standard memorization configuration:\n",
    "        - Words -> Verse (based on word length)\n",
    "        - Verse -> Chapter (equal weights)\n",
    "        - Context windows for words\n",
    "        \"\"\"\n",
    "        for chapter_id in self.get_nodes_by_type(\"chapter\"):\n",
    "            chapter : Chapter = self.quran[ NIP.get_chapter_key(chapter_id) ]\n",
    "            for verse in chapter.verses:\n",
    "                # Verse to chapter memorization edges\n",
    "                self.edge_manager.add_knowledge_edge(\n",
    "                    f\"{NIG.for_verse(verse)}:memorization\",\n",
    "                    f\"{NIG.for_chapter(chapter)}:memorization\",\n",
    "                    Distribution.auto(weight=verse.get_letters_count())\n",
    "                )\n",
    "\n",
    "                # Word specific\n",
    "                for word in (w for w in verse.words if not w.is_end_word()):\n",
    "                    # Word to verse memorization edges\n",
    "                    self.edge_manager.add_knowledge_edge(\n",
    "                        f\"{NIG.for_word_instance(word, verse)}:memorization\",\n",
    "                        f\"{NIG.for_verse(verse)}:memorization\",\n",
    "                        Distribution.auto(weight=word.get_letters_count())\n",
    "                    )\n",
    "\n",
    "\n",
    "                # Contextual memorization for words\n",
    "                self.edge_manager.add_gaussian_window_edges(\n",
    "                    [f\"{NIG.for_word_instance(w, verse)}:memorization\" for w in verse.words if not w.is_end_word()],\n",
    "                    window_size=3,\n",
    "                    base_weight=0.5,\n",
    "                    std_scale=0.15\n",
    "                )\n",
    "\n",
    "    def setup_tajweed_learning(self):\n",
    "        \"\"\"\n",
    "        Tajweed learning configuration:\n",
    "        - Strong impact on memorization\n",
    "        - Affected by neighboring words (for rules spanning multiple words)\n",
    "        \"\"\"\n",
    "        from itertools import tee, zip_longest\n",
    "\n",
    "        # Tajweed impacts memorization\n",
    "        for verse_id in self.get_nodes_by_type(\"verse\"):\n",
    "            verse = self.quran[NIP.get_verse_key(verse_id)]\n",
    "\n",
    "            iter_current, iter_next = tee(verse.words)\n",
    "            next(iter_next, None)\n",
    "\n",
    "            for current_word, next_word in zip_longest(iter_current, iter_next, fillvalue=None):\n",
    "                word_instance_id = NIG.for_word_instance(current_word, verse)\n",
    "                if self.has_tajweed_rules(word_instance_id):\n",
    "                    self.edge_manager.add_knowledge_edge(\n",
    "                        f\"{word_instance_id}:tajweed\",\n",
    "                        f\"{word_instance_id}:memorization\",\n",
    "                        Distribution.normal(mean=0.7, std=0.1), # strong impact\n",
    "                    )\n",
    "\n",
    "                    # Connect neighboring tajweed nodes (for rules spanning words)\n",
    "                    if next_word:\n",
    "                        self.edge_manager.add_knowledge_edge(\n",
    "                            f\"{word_instance_id}:tajweed\",\n",
    "                            f\"{NIG.for_word_instance(next_word, verse)}:tajweed\",\n",
    "                            Distribution.normal(mean=0.3, std=0.1), # weak impact\n",
    "                        )\n",
    "\n",
    "    def get_all_translatable_nodes(self):\n",
    "        return self.node_manager.get_nodes_by_type(\"word_instance\") | self.node_manager.get_nodes_by_type(\"verse\")\n",
    "\n",
    "    def get_duplicated_verses(self) -> list[tuple[str, list[str]]]:\n",
    "        h = {}\n",
    "        # for verse in quran[:,:]:\n",
    "        for verse_id in self.get_nodes_by_type(\"verse\"):\n",
    "            verse = self.quran[NIP.get_verse_key(verse_id)]\n",
    "            k = verse.text_uthmani_simple\n",
    "            assert k is not None, f\"we need text_uthmani_simple to be defined\"\n",
    "            h[k] = h.get(k, []) + [verse.verse_key]\n",
    "\n",
    "        h = dict(filter(lambda x:len( x[1]) > 1, h.items()))\n",
    "        return sorted(h.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "    def setup_translation_understanding(self):\n",
    "        \"\"\"\n",
    "        Translation learning configuration:\n",
    "        - Word meanings contribute to verse meaning\n",
    "        - Understanding impacts memorization\n",
    "        - Related verses strengthen each other\n",
    "        \"\"\"\n",
    "\n",
    "        for chapter_id in self.get_nodes_by_type(\"chapter\"):\n",
    "            chapter = self.quran[ NIP.get_chapter_key(chapter_id) ]\n",
    "            for verse in chapter.verses:\n",
    "\n",
    "                # Verse translation to chapter translation\n",
    "                self.edge_manager.add_knowledge_edge(\n",
    "                    f\"{NIG.for_verse(verse)}:translation\",\n",
    "                    f\"{chapter_id}:translation\",\n",
    "                    Distribution.auto(weight=verse.get_words_count())\n",
    "                )\n",
    "\n",
    "                # Word translation to verse translation\n",
    "                for word in (w for w in verse.words if not w.is_end_word()):\n",
    "                    self.edge_manager.add_knowledge_edge(\n",
    "                        f\"{NIG.for_word_instance(word, verse)}:translation\",\n",
    "                        f\"{NIG.for_verse(verse)}:translation\",\n",
    "                        Distribution.auto(weight=word.get_letters_count())\n",
    "                    )\n",
    "\n",
    "                    # Word instance to word memorization edges\n",
    "                    self.edge_manager.add_knowledge_edge(\n",
    "                        f\"{NIG.for_word_instance(word, verse)}:translation\",\n",
    "                        f\"{NIG.for_word(word)}:translation\",\n",
    "                        Distribution.normal(mean=0.9, std=0.1) # very high impact (basically same word)\n",
    "                    )\n",
    "\n",
    "\n",
    "        # Translation helps memorization\n",
    "        for node_id in self.get_all_translatable_nodes():\n",
    "            self.edge_manager.add_knowledge_edge(\n",
    "                f\"{node_id}:translation\",\n",
    "                f\"{node_id}:memorization\",\n",
    "                Distribution.normal(mean=0.4, std=0.15)\n",
    "            )\n",
    "\n",
    "        # Strongly connect duplicated verses\n",
    "        for (verse, verse_pairs) in self.get_duplicated_verses():\n",
    "            for i in range(len(verse_pairs)):\n",
    "                for j in range(i+1, len(verse_pairs)):\n",
    "                    v1, v2 = verse_pairs[i], verse_pairs[j]\n",
    "                    self.edge_manager.add_bidirectional_knowledge_edge(\n",
    "                        f\"{NIG.for_verse(v1)}:translation\",\n",
    "                        f\"{NIG.for_verse(v2)}:translation\",\n",
    "                        Distribution.normal(mean=0.9, std=0.1)\n",
    "                    )\n",
    "\n",
    "    def setup_deep_understanding(self):\n",
    "        \"\"\"\n",
    "        Deep understanding configuration:\n",
    "        - Translation -> Tafsir\n",
    "        - Root meaning impacts understanding\n",
    "        - Related verses strengthen understanding\n",
    "        \"\"\"\n",
    "        # Translation aids tafsir\n",
    "        for verse_id in (self.get_nodes_by_type(\"verse\") & self.node_manager.get_nodes_by_metadata(\"has_tafsir\")):\n",
    "            self.edge_manager.add_knowledge_edge(\n",
    "                f\"{verse_id}:translation\",\n",
    "                f\"{verse_id}:tafsir\",\n",
    "                Distribution.normal(mean=0.3, std=0.1)\n",
    "            )\n",
    "\n",
    "        # Root meanings impact word understanding\n",
    "        for lemma in self.get_nodes_by_type(\"lemma\"):\n",
    "            root = self.get_word_root(lemma)\n",
    "            if root:\n",
    "                self.edge_manager.add_knowledge_edge(\n",
    "                    f\"{root}:meaning\",\n",
    "                    f\"{lemma}:translation\",\n",
    "                    Distribution.beta(alpha=4, beta=2) # Strong positive skew\n",
    "                )\n",
    "\n",
    "    def compile(self) -> None:\n",
    "        \"\"\"\n",
    "        Finalize the knowledge graph by computing all pending weights\n",
    "        and performing any necessary validations.\n",
    "\n",
    "        Should be called after all setup_* methods and before saving/using the graph.\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If compile is called more than once\n",
    "            ValueError: If there are invalid edge configurations\n",
    "        \"\"\"\n",
    "        if self._is_compiled:\n",
    "            raise RuntimeError(\"Knowledge graph has already been compiled\")\n",
    "\n",
    "        # Attribute weights to all pending edges\n",
    "        self.edge_manager.compile()\n",
    "\n",
    "        # Validate final graph state\n",
    "        self._validate_compiled_graph()\n",
    "\n",
    "        self._is_compiled = True\n",
    "\n",
    "    def _validate_compiled_graph(self) -> None:\n",
    "        \"\"\"\n",
    "        Perform final validation checks on the compiled graph.\n",
    "        Add any necessary validation logic here.\n",
    "        \"\"\"\n",
    "        # Example validation: ensure no edges are missing weights\n",
    "        for src, dst, data in self.G.edges(data=True):\n",
    "            if \"dist\" not in data:\n",
    "                is_exception = (\n",
    "                    data.get(\"type\") == \"dependency\" # Allow dependency edges to be missing weights, they are not used for learning but scheduling\n",
    "                )\n",
    "                if not is_exception:\n",
    "                    raise ValueError(f\"Found edge missing weight distribution after compilation: {src} -> {dst} {{{data}}}\")\n",
    "\n",
    "    def save(self, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the compiled knowledge graph to a file.\n",
    "\n",
    "        Args:\n",
    "            filename: Path to save the GraphML file\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If save is attempted before compilation\n",
    "        \"\"\"\n",
    "        if not self._is_compiled:\n",
    "            raise RuntimeError(\n",
    "                \"Cannot save uncompiled knowledge graph. Call compile() first\"\n",
    "            )\n",
    "        if filename.endswith(\".graphml\"):\n",
    "            # Save as GraphML\n",
    "            nx.write_graphml(self.G, filename)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Use .graphml or .json\")\n",
    "\n",
    "    def setup_experimental_learning(self):\n",
    "        \"\"\"\n",
    "        Experimental learning strategies to test hypotheses\n",
    "        \"\"\"\n",
    "        # Hypothesis: Memorization is strengthened by multiple knowledge types\n",
    "        def setup_multimodal_learning():\n",
    "            for verse_id in self.get_nodes_by_type(\"verse\"):\n",
    "                # Create composite effect from different knowledge types\n",
    "                knowledge_types = ['translation', 'tajweed', 'contextual_memorization']\n",
    "                for kt in knowledge_types:\n",
    "                    self.edge_manager.add_knowledge_edge(\n",
    "                        f\"{verse_id}:{kt}\",\n",
    "                        f\"{verse_id}:memorization\",\n",
    "                        distribution=\"normal\",\n",
    "                        m=0.3,  # Individual effects are moderate\n",
    "                        s=0.1\n",
    "                    )\n",
    "\n",
    "        # Hypothesis: Learning is affected by position in chapter\n",
    "        def setup_position_based_learning():\n",
    "            for chapter_id in self.get_nodes_by_type(\"chapter\"):\n",
    "                verses = self.get_chapter_verses(chapter_id)\n",
    "                for i, verse_id in enumerate(verses):\n",
    "                    position_factor = 1 - (i / len(verses))  # Stronger at start\n",
    "                    self.edge_manager.add_knowledge_edge(\n",
    "                        f\"{verse_id}:memorization\",\n",
    "                        f\"{chapter_id}:memorization\",\n",
    "                        distribution=\"normal\",\n",
    "                        m=position_factor * 0.5,\n",
    "                        s=0.1\n",
    "                    )\n",
    "\n",
    "        # Hypothesis: Understanding spreads differently than memorization\n",
    "        def setup_differential_propagation():\n",
    "            for verse_id in self.get_nodes_by_type(\"verse\"):\n",
    "                words = self.get_verse_words(verse_id)\n",
    "\n",
    "                # Understanding propagates bottom-up\n",
    "                self.edge_manager.add_cascading_edges(\n",
    "                    [f\"{w}:translation\" for w in words],\n",
    "                    f\"{verse_id}:translation\",\n",
    "                    cascade_type=\"bottom_up\",\n",
    "                    base_weight=0.4\n",
    "                )\n",
    "\n",
    "                # Memorization propagates top-down\n",
    "                self.edge_manager.add_cascading_edges(\n",
    "                    [f\"{w}:memorization\" for w in words],\n",
    "                    f\"{verse_id}:memorization\",\n",
    "                    cascade_type=\"top_down\",\n",
    "                    base_weight=0.6\n",
    "                )\n",
    "\n",
    "    def setup_grammar_nodes(self):\n",
    "        for word_id in self.get_nodes_by_type(\"word\"):\n",
    "            lemma_ids = self.node_manager.get_related_nodes(word_id, successor_type=\"lemma\")\n",
    "            # assert len(lemma_ids) <= 1, f\"Word {word_id} has multiple lemmas: {lemma_ids}\"\n",
    "            for lemma_id in lemma_ids:\n",
    "                _, lemma = lemma_id.split(\":\")\n",
    "\n",
    "                # Add translation edge between word and lemma\n",
    "                self.edge_manager.add_bidirectional_knowledge_edge(\n",
    "                    f\"{word_id}:translation\",\n",
    "                    f\"{lemma_id}:translation\",\n",
    "                    Distribution.auto(weight=len(lemma))\n",
    "                )\n",
    "\n",
    "                # attempt to look for root\n",
    "                root_ids = self.node_manager.get_related_nodes(word_id, successor_type=\"root\")\n",
    "                assert len(root_ids) <= 1, f\"Word {word_id} has multiple roots: {root_ids}\"\n",
    "                if root_ids:\n",
    "                    # Add translation edge between lemma and root\n",
    "                    self.edge_manager.add_bidirectional_knowledge_edge(\n",
    "                        f\"{lemma_id}:translation\",\n",
    "                        f\"{root_ids[0]}:meaning\"\n",
    "                    )\n",
    "\n",
    "                # FIXME: can we have a root but no lemma?\n",
    "                assert not self.node_manager.get_related_nodes(word_id, successor_type=\"root\"), f\"Word {word_id} has neither root nor lemma\"\n",
    "\n",
    "\n",
    "# Example usage in notebook:\n",
    "# graph = nx.read_graphml(\"quran_dependency_fatiha.graphml\")\n",
    "# graph = nx.read_graphml(\"quran_dependency_medium.graphml\")\n",
    "# graph = nx.read_graphml(\"quran_dependency_big.graphml\")\n",
    "graph = nx.read_graphml(\"quran_dependency_full.graphml\")\n",
    "exp = KnowledgeExperiments(graph, quran)\n",
    "\n",
    "# Try different configurations\n",
    "exp.setup_standard_memorization()\n",
    "exp.setup_tajweed_learning()\n",
    "# exp.setup_translation_understanding()\n",
    "exp.setup_deep_understanding()\n",
    "exp.setup_grammar_nodes()\n",
    "\n",
    "# # Test experimental hypotheses\n",
    "# exp.setup_experimental_learning()\n",
    "\n",
    "# Save different configurations\n",
    "# nx.write_graphml(graph, \"quran_knowledge_medium.graphml\")\n",
    "# nx.write_graphml(graph, \"quran_knowledge_fatiha.graphml\")\n",
    "exp.compile()\n",
    "# exp.save(\"quran_knowledge_fatiha.graphml\")\n",
    "# exp.save(\"quran_knowledge_medium2.graphml\")\n",
    "# exp.save(\"quran_knowledge_big.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iqrah.graph.identifiers import NodeIdentifierParser, NodeType\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ---------------------------\n",
    "# Personalization by node type\n",
    "# ---------------------------\n",
    "NODE_TYPE_WEIGHTS = {\n",
    "    NodeType.ROOT: 3.0,\n",
    "    NodeType.LEMMA: 2.5,\n",
    "    NodeType.CHAPTER: 2.0,\n",
    "    NodeType.VERSE: 1.5,\n",
    "    NodeType.WORD: 1.0,\n",
    "    NodeType.WORD_INSTANCE: 0.5,\n",
    "}\n",
    "\n",
    "def create_personalized_nstart(graph: nx.DiGraph) -> dict[str, float]:\n",
    "    if not graph.nodes():\n",
    "        return {}\n",
    "    node_weights: dict[str, float] = {}\n",
    "    total = 0.0\n",
    "    for node_id in tqdm(graph.nodes(), desc=\"Calculating nstart weights\", leave=False):\n",
    "        try:\n",
    "            node_type, _ = NodeIdentifierParser.parse(node_id)\n",
    "            w = NODE_TYPE_WEIGHTS.get(node_type, 1.0)\n",
    "        except Exception:\n",
    "            w = 1.0\n",
    "        if w < 0 or not np.isfinite(w):\n",
    "            w = 0.0\n",
    "        node_weights[node_id] = w\n",
    "        total += w\n",
    "    if total == 0.0:\n",
    "        u = 1.0 / max(1, len(node_weights))\n",
    "        return {n: u for n in node_weights}\n",
    "    return {n: w / total for n, w in node_weights.items()}\n",
    "\n",
    "# ---------------------------\n",
    "# Edge weight expectation\n",
    "# ---------------------------\n",
    "def _expected_edge_weight(data: dict) -> float:\n",
    "    \"\"\"Analytic expectation; keep weights nonnegative (and ‚â§1 if probability-like).\"\"\"\n",
    "    dist = data.get(\"dist\")\n",
    "    if dist == \"normal\":\n",
    "        m = float(data.get(\"m\", 0.0))\n",
    "        w = np.clip(m, 0.0, 1.0)  # treat as probability\n",
    "    elif dist == \"beta\":\n",
    "        a = float(data.get(\"a\", 1.0))\n",
    "        b = float(data.get(\"b\", 1.0))\n",
    "        denom = a + b\n",
    "        w = (a / denom) if denom > 0 else 0.0\n",
    "    elif dist in (\"auto\", \"constant\"):\n",
    "        w = float(data.get(\"weight\", 1.0))\n",
    "        w = np.clip(w, 0.0, 1.0) if data.get(\"probability_like\", True) else max(0.0, w)\n",
    "    else:\n",
    "        return 1.0\n",
    "    if not np.isfinite(w) or w < 0:\n",
    "        return 0.0\n",
    "    return float(w)\n",
    "\n",
    "def _normalize_dist(vec: dict[str, float] | None, universe: list[str]) -> dict[str, float]:\n",
    "    \"\"\"Ensure nonnegative and sum to 1 over the universe; fallback to uniform.\"\"\"\n",
    "    out = {}\n",
    "    total = 0.0\n",
    "    for n in universe:\n",
    "        v = float(vec.get(n, 0.0)) if vec is not None else 0.0\n",
    "        if not np.isfinite(v) or v < 0:\n",
    "            v = 0.0\n",
    "        out[n] = v\n",
    "        total += v\n",
    "    if total == 0.0:\n",
    "        u = 1.0 / max(1, len(universe))\n",
    "        return {n: u for n in universe}\n",
    "    return {n: v / total for n, v in out.items()}\n",
    "\n",
    "# ---------------------------\n",
    "# Log01 normalization util\n",
    "# ---------------------------\n",
    "def _log01_array(arr: np.ndarray, scale: float | None = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Clip to >=0, apply log1p(arr*scale), then min-max to [0,1].\n",
    "    scale defaults to 1/median to avoid collapsing the bulk.\n",
    "    \"\"\"\n",
    "    arr = np.clip(arr, 0.0, None)\n",
    "    if arr.size == 0:\n",
    "        return arr\n",
    "\n",
    "    if scale is None:\n",
    "        positives = arr[arr > 0]\n",
    "        med = np.median(positives) if positives.size else 0.0\n",
    "        scale = (1.0 / med) if med > 0 else 1e9\n",
    "\n",
    "    x = np.log1p(arr * scale)\n",
    "    xmin = np.min(x)\n",
    "    denom = np.ptp(x)  # == x.max() - x.min()\n",
    "\n",
    "    if not np.isfinite(xmin) or not np.isfinite(denom) or denom == 0:\n",
    "        return np.zeros_like(arr)\n",
    "\n",
    "    return (x - xmin) / denom\n",
    "\n",
    "# ---------------------------\n",
    "# Main scoring (stores only log01 scores)\n",
    "# ---------------------------\n",
    "def calculate_knowledge_scores(\n",
    "    G: nx.DiGraph,\n",
    "    alpha: float = 0.85,\n",
    "    max_iter: int = 50000,\n",
    "    nstart_foundational: dict[str, float] | None = None,\n",
    "    nstart_influence: dict[str, float] | None = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Computes two PageRank vectors on a knowledge-only graph, log01-normalizes them,\n",
    "    and writes ONLY:\n",
    "      - G.nodes[n][\"foundational_score\"]\n",
    "      - G.nodes[n][\"influence_score\"]\n",
    "    \"\"\"\n",
    "    # Build knowledge edges with analytic expectations\n",
    "    knowledge_edges = []\n",
    "    for u, v, data in tqdm(G.edges(data=True), desc=\"Building knowledge graph\", leave=False):\n",
    "        # if data.get(\"type\") == \"dependency\":\n",
    "            # continue\n",
    "        w = _expected_edge_weight(data)\n",
    "        if w > 0.0:\n",
    "            knowledge_edges.append((u, v, {\"weight\": w}))\n",
    "\n",
    "    knowledge_graph = nx.DiGraph()\n",
    "    knowledge_graph.add_nodes_from(G.nodes())\n",
    "    knowledge_graph.add_edges_from(knowledge_edges)\n",
    "\n",
    "    # Personalized vectors + dangling vectors\n",
    "    nodes_list = list(knowledge_graph.nodes())\n",
    "    pers_found = _normalize_dist(nstart_foundational, nodes_list)\n",
    "    pers_infl  = _normalize_dist(nstart_influence, nodes_list)\n",
    "\n",
    "    # Foundational PageRank\n",
    "    pr_found = nx.pagerank(\n",
    "        knowledge_graph,\n",
    "        alpha=alpha,\n",
    "        personalization=pers_found,\n",
    "        dangling=pers_found,\n",
    "        weight=\"weight\",\n",
    "        max_iter=max_iter,\n",
    "    )\n",
    "    # Influence PageRank (reverse)\n",
    "    pr_infl = nx.pagerank(\n",
    "        knowledge_graph.reverse(copy=False),\n",
    "        alpha=alpha,\n",
    "        personalization=pers_infl,\n",
    "        dangling=pers_infl,\n",
    "        weight=\"weight\",\n",
    "        max_iter=max_iter,\n",
    "    )\n",
    "\n",
    "    # Convert to arrays in a consistent node order for normalization\n",
    "    f_arr = np.array([float(pr_found[n]) for n in nodes_list], dtype=float)\n",
    "    i_arr = np.array([float(pr_infl[n])  for n in nodes_list], dtype=float)\n",
    "\n",
    "    # Clip tiny numerical negatives (shouldn't appear) and log01-normalize\n",
    "    f_norm = _log01_array(np.clip(f_arr, 0.0, None))\n",
    "    i_norm = _log01_array(np.clip(i_arr, 0.0, None))\n",
    "\n",
    "    # Write ONLY the final normalized scores\n",
    "    for n, f, i in zip(nodes_list, f_norm, i_norm):\n",
    "        G.nodes[n][\"foundational_score\"] = float(f)\n",
    "        G.nodes[n][\"influence_score\"]    = float(i)\n",
    "\n",
    "# ============================\n",
    "# Usage\n",
    "# ============================\n",
    "# 1) Get your graph\n",
    "G = exp.G\n",
    "\n",
    "# 2) Personalization for foundational; keep influence unbiased\n",
    "nstart_foundational = create_personalized_nstart(G)\n",
    "nstart_influence = None\n",
    "\n",
    "# 3) Compute and store ONLY the two normalized scores\n",
    "calculate_knowledge_scores(\n",
    "    G,\n",
    "    alpha=0.85,\n",
    "    max_iter=100_000,\n",
    "    nstart_foundational=nstart_foundational,\n",
    "    nstart_influence=nstart_influence,\n",
    ")\n",
    "\n",
    "# (Optional) Quick peek\n",
    "some_nodes = list(G.nodes())[:5]\n",
    "for n in some_nodes:\n",
    "    print(n, \"found:\", G.nodes[n].get(\"foundational_score\"),\n",
    "             \"infl:\",  G.nodes[n].get(\"influence_score\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(nid, d) for (nid, d) in G.nodes(data=True)][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(G.predecessors(\"LEMMA:Ÿà\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node_id, node_data in sorted(\n",
    "    # [(nid, d) for (nid, d) in G.nodes(data=True) ], key=lambda x: x[1].get(\"influence_score\", 0), reverse=True\n",
    "    [(nid, d) for (nid, d) in G.nodes(data=True) ], key=lambda x: x[1].get(\"foundational_score\", 0), reverse=True\n",
    ")[:1000]:\n",
    "# ):\n",
    "    foundational = node_data.get(\"foundational_score\", \"N/A\")\n",
    "    influence = node_data.get(\"influence_score\", \"N/A\")\n",
    "    print(\n",
    "        f\"Node: {node_id}, Foundational: {foundational:.6f}, Influence: {influence:.6f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for _, n in G.nodes(data=True):\n",
    "    if \"foundational_score\" in n and \"influence_score\" in n:\n",
    "        rows.append({\n",
    "            \"foundational_score\": n.get(\"foundational_score\"),\n",
    "            \"influence_score\": n.get(\"influence_score\"),\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "df[\"foundational_score\"].hist(bins=500, alpha=0.7, label=\"Foundational\")\n",
    "df[\"influence_score\"].hist(bins=500, alpha=0.7, label=\"Influence\")\n",
    "plt.xlabel(\"log01 score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of log01 scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbor2\n",
    "from iqrah.quran_api.models import TranslationByWord, TransliterationByWord\n",
    "import zstandard as zstd\n",
    "import os\n",
    "\n",
    "def export_graph_to_cbor(G, output_path=\"iqrah-graph-v1.0.0.cbor.zst\"):\n",
    "    \"\"\"Export NetworkX graph to compressed CBOR sequence with progress info\"\"\"\n",
    "\n",
    "    print(f\"üöÄ Starting export of {len(G.nodes)} nodes, {len(G.edges)} edges...\")\n",
    "\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        cctx = zstd.ZstdCompressor(level=9).stream_writer(f)\n",
    "        enc = cbor2.CBOREncoder(cctx)\n",
    "\n",
    "        # Header with metadata\n",
    "        header = {\n",
    "            \"v\": 1,\n",
    "            \"graph\": {\n",
    "                \"directed\": G.is_directed(),\n",
    "                \"multi\": G.is_multigraph(),\n",
    "                \"node_count\": len(G.nodes),\n",
    "                \"edge_count\": len(G.edges)\n",
    "            }\n",
    "        }\n",
    "        enc.encode(header)\n",
    "        print(f\"üìã Header written: {len(G.nodes)} nodes, {len(G.edges)} edges\")\n",
    "\n",
    "        # Export nodes directly\n",
    "        node_count = 0\n",
    "        for node_id, attrs in G.nodes(data=True):\n",
    "            match attrs.get('type', 'unknown'):\n",
    "                case 'word_instance':\n",
    "                    word_key = NodeIdentifierParser.get_word_instance_key(node_id)\n",
    "                    word : Word = quran[word_key]\n",
    "                    translation : TranslationByWord = word.translation\n",
    "                    transliteration : TransliterationByWord = word.transliteration\n",
    "                    attrs['audio_url'] = word.audio_url\n",
    "                    attrs['arabic'] = word.text_uthmani\n",
    "                    attrs['translation'] = translation.text\n",
    "                    attrs['transliteration'] = transliteration.text\n",
    "\n",
    "                case 'verse':\n",
    "                    verse_key = NodeIdentifierParser.get_verse_key(node_id)\n",
    "                    verse : Verse = quran[verse_key]\n",
    "                    attrs['arabic'] = verse.text_uthmani\n",
    "                    # attrs['translation'] = verse.translation\n",
    "\n",
    "            enc.encode({\n",
    "                \"t\": \"node\",\n",
    "                \"id\": str(node_id),\n",
    "                \"a\": dict(attrs)  # NetworkX attributes as-is\n",
    "            })\n",
    "            node_count += 1\n",
    "\n",
    "            if node_count % 1000 == 0:\n",
    "                print(f\"üì¶ Exported {node_count} nodes...\")\n",
    "\n",
    "        print(f\"‚úÖ All {node_count} nodes exported\")\n",
    "\n",
    "        # Export edges directly\n",
    "        edge_count = 0\n",
    "        for u, v, attrs in G.edges(data=True):\n",
    "            # if 'type' in attrs:\n",
    "                # attrs =\n",
    "\n",
    "            enc.encode({\n",
    "                \"t\": \"edge\",\n",
    "                \"u\": str(u),\n",
    "                \"v\": str(v),\n",
    "                \"a\": dict(attrs)  # NetworkX attributes as-is\n",
    "            })\n",
    "            edge_count += 1\n",
    "\n",
    "            if edge_count % 1000 == 0:\n",
    "                print(f\"üîó Exported {edge_count} edges...\")\n",
    "\n",
    "        print(f\"‚úÖ All {edge_count} edges exported\")\n",
    "        cctx.flush()\n",
    "\n",
    "    # File size report\n",
    "    file_size = os.path.getsize(output_path)\n",
    "    print(f\"üéØ Export complete!\")\n",
    "    print(f\"üìÅ File: {output_path}\")\n",
    "    print(f\"üìä Size: {file_size / 1024 / 1024:.2f} MB ({file_size:,} bytes)\")\n",
    "\n",
    "\n",
    "def inspect_exported_graph(file_path, sample_size=10):\n",
    "    \"\"\"Fixed inspection - decode records in loop\"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"\\nüîç Inspecting: {file_path}\")\n",
    "    print(f\"üìÅ Size: {file_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            dctx = zstd.ZstdDecompressor().stream_reader(f)\n",
    "            decoder = cbor2.CBORDecoder(dctx)\n",
    "\n",
    "            # Read header\n",
    "            header = decoder.decode()\n",
    "            print(f\"\\nüìã Header:\")\n",
    "            print(f\"   Version: {header['v']}\")\n",
    "            print(f\"   Nodes: {header['graph']['node_count']:,}\")\n",
    "            print(f\"   Edges: {header['graph']['edge_count']:,}\")\n",
    "\n",
    "            # Sample records with manual iteration\n",
    "            print(f\"\\nüî¨ Sample Records (first {sample_size}):\")\n",
    "\n",
    "            node_types = {}\n",
    "            edge_attrs = {}\n",
    "            count = 0\n",
    "\n",
    "            try:\n",
    "                while record := decoder.decode():\n",
    "                    count += 1\n",
    "\n",
    "                    print(f\"[{count}] {record}\")\n",
    "                    if record[\"t\"] == \"node\":\n",
    "                        attrs = record[\"a\"]\n",
    "                        node_type = attrs.get(\"type\", \"unknown\")\n",
    "                        node_types[node_type] = node_types.get(node_type, 0) + 1\n",
    "\n",
    "                        if count <= 3:\n",
    "                            print(f\"   Node: {record['id']}\")\n",
    "                            print(f\"         attrs: {attrs}\")\n",
    "\n",
    "                    elif record[\"t\"] == \"edge\":\n",
    "                        attrs = record[\"a\"]\n",
    "                        for key in attrs.keys():\n",
    "                            edge_attrs[key] = edge_attrs.get(key, 0) + 1\n",
    "\n",
    "                        if count <= 3:\n",
    "                            print(f\"   Edge: {record['u']} ‚Üí {record['v']}\")\n",
    "                            print(f\"         attrs: {attrs}\")\n",
    "\n",
    "            except EOFError:\n",
    "                print(f\"‚úÖ Reached end of file after {count} records\")\n",
    "                pass  # End of file\n",
    "\n",
    "            print(f\"\\nüìä Sample Statistics:\")\n",
    "            print(f\"   Node types: {dict(sorted(node_types.items()))}\")\n",
    "            print(f\"   Edge attribute keys: {dict(sorted(edge_attrs.items()))}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Inspection failed: {e}\")\n",
    "\n",
    "\n",
    "def run_export(G):\n",
    "    \"\"\"Run the complete export process\"\"\"\n",
    "    print(f\"üéØ NetworkX Graph Export\")\n",
    "    print(f\"üìä Graph: {len(G.nodes):,} nodes, {len(G.edges):,} edges\")\n",
    "    print(f\"üîÑ Directed: {G.is_directed()}, Multi: {G.is_multigraph()}\")\n",
    "\n",
    "    # Export\n",
    "    output_file = \"iqrah-graph-v1.0.1.cbor.zst\"\n",
    "    export_graph_to_cbor(G, output_file)\n",
    "\n",
    "    # Inspect results\n",
    "    print(\"=\" * 50)\n",
    "    inspect_exported_graph(output_file, 1000)\n",
    "\n",
    "    print(f\"\\nüöÄ Ready for Sprint 4!\")\n",
    "    return output_file\n",
    "\n",
    "# Run it\n",
    "exported_file = run_export(exp.G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if edges have distribution parameters\n",
    "sample_edges = list(exp.G.edges(data=True))\n",
    "for u, v, attrs in sample_edges:\n",
    "    if attrs:\n",
    "        print(f\"Edge {u} -> {v}: {attrs}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"No edge attributes found in first 10 edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quran[\"1:2\"]\n",
    "# exp.G[\"LEMMA:ÿßŸÑ:translation\"]\n",
    "exp.G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## version 2 (not working)\n",
    "\n",
    "import networkx as nx\n",
    "import logging\n",
    "import json\n",
    "from typing import Dict, List, Set, Tuple, Optional, Any\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "from iqrah.graph.identifiers import NIG, NIP\n",
    "from iqrah.graph.knowledge import Distribution, KnowledgeEdgeManager\n",
    "from iqrah.graph.node_manager import NodeManager\n",
    "from iqrah.quran_api.models import Quran, Chapter, Verse, Word\n",
    "\n",
    "\n",
    "class KnowledgeGraphBuilder:\n",
    "    \"\"\"\n",
    "    Builds a knowledge graph for Quranic learning with focus on:\n",
    "    - Memorization relationships\n",
    "    - Translation understanding\n",
    "    - Grammar connections\n",
    "\n",
    "    This is a streamlined version focused on core functionality.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph: nx.DiGraph, quran: Quran):\n",
    "        \"\"\"\n",
    "        Initialize the knowledge graph builder.\n",
    "\n",
    "        Args:\n",
    "            graph: Base dependency graph (should already contain nodes)\n",
    "            quran: Quran data model\n",
    "        \"\"\"\n",
    "        self.G = graph\n",
    "        self.quran = quran\n",
    "        self.edge_manager = KnowledgeEdgeManager(graph)\n",
    "        self.node_manager = NodeManager(graph)\n",
    "        self._is_compiled = False\n",
    "\n",
    "        # Initialize statistics\n",
    "        self.stats = Counter()\n",
    "\n",
    "        logger.info(f\"Initialized KnowledgeGraphBuilder with graph of {self.G.number_of_nodes()} nodes and {self.G.number_of_edges()} edges\")\n",
    "\n",
    "    def get_nodes_by_type(self, node_type: str) -> Set[str]:\n",
    "        \"\"\"Get all nodes of a specific type\"\"\"\n",
    "        return self.node_manager.get_nodes_by_type(node_type)\n",
    "\n",
    "    def get_verse_words(self, verse_id: str) -> List[str]:\n",
    "        \"\"\"Get all word instance IDs for a verse\"\"\"\n",
    "        return self.node_manager.get_verse_words(verse_id)\n",
    "\n",
    "    def get_chapter_verses(self, chapter_id: str) -> List[str]:\n",
    "        \"\"\"Get all verse IDs for a chapter\"\"\"\n",
    "        return self.node_manager.get_chapter_verses(chapter_id)\n",
    "\n",
    "    def get_word_root(self, word_id: str, cutoff: int = 3) -> Optional[str]:\n",
    "        \"\"\"Get root of a word by traversing the graph\"\"\"\n",
    "        for path in nx.all_simple_paths(self.G, word_id,\n",
    "                                      self.node_manager.get_nodes_by_type(\"root\"),\n",
    "                                      cutoff=cutoff):\n",
    "            return path[-1]  # Return first found root\n",
    "        return None\n",
    "\n",
    "    def get_all_translatable_nodes(self) -> Set[str]:\n",
    "        \"\"\"Get all nodes that can have translation knowledge\"\"\"\n",
    "        return (self.node_manager.get_nodes_by_type(\"word_instance\") |\n",
    "                self.node_manager.get_nodes_by_type(\"verse\") |\n",
    "                self.node_manager.get_nodes_by_type(\"chapter\"))\n",
    "\n",
    "    def get_duplicated_verses(self) -> List[Tuple[str, List[str]]]:\n",
    "        \"\"\"Find verses with identical text for connecting\"\"\"\n",
    "        verse_map = {}\n",
    "        for verse_id in self.get_nodes_by_type(\"verse\"):\n",
    "            verse = self.quran[NIP.get_verse_key(verse_id)]\n",
    "            text = verse.text_uthmani_simple\n",
    "            if text is None:\n",
    "                continue\n",
    "            verse_map[text] = verse_map.get(text, []) + [verse.verse_key]\n",
    "\n",
    "        duplicates = {text: verses for text, verses in verse_map.items() if len(verses) > 1}\n",
    "        return sorted(duplicates.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "    def calculate_word_complexity(self, word: Word) -> float:\n",
    "        \"\"\"\n",
    "        Calculate linguistic complexity of a word based on basic factors.\n",
    "\n",
    "        Args:\n",
    "            word: Word to analyze\n",
    "\n",
    "        Returns:\n",
    "            Complexity score (0.0-1.0)\n",
    "        \"\"\"\n",
    "        # Simple complexity based on length\n",
    "        text = word.text_uthmani or \"\"\n",
    "        if not text:\n",
    "            return 0.5\n",
    "\n",
    "        # Length complexity (longer words are harder)\n",
    "        char_count = word.get_letters_count()\n",
    "        length_complexity = min(1.0, char_count / 10.0)\n",
    "\n",
    "        # Position factor (words at beginning or end are more memorable)\n",
    "        position_complexity = 0.5\n",
    "\n",
    "        # Combine factors\n",
    "        complexity = 0.7 * length_complexity + 0.3 * position_complexity\n",
    "\n",
    "        return min(1.0, complexity)\n",
    "\n",
    "    def build_all(self) -> None:\n",
    "        \"\"\"\n",
    "        Build all knowledge connections.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        logger.info(\"Building knowledge graph connections\")\n",
    "\n",
    "        # Step 1: Standard memorization\n",
    "        logger.info(\"Building memorization connections\")\n",
    "        self.build_memorization_connections()\n",
    "\n",
    "        # Step 2: Translation understanding\n",
    "        logger.info(\"Building translation connections\")\n",
    "        self.build_translation_connections()\n",
    "\n",
    "        # Step 3: Grammar connections\n",
    "        logger.info(\"Building grammar connections\")\n",
    "        self.build_grammar_connections()\n",
    "\n",
    "        # Step 4: Connect learning dimensions\n",
    "        logger.info(\"Connecting learning dimensions\")\n",
    "        self.connect_learning_dimensions()\n",
    "\n",
    "        # Finalize\n",
    "        self.compile()\n",
    "\n",
    "        # Report statistics\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Knowledge graph built in {elapsed_time:.2f}s\")\n",
    "        logger.info(f\"Created {self.stats['edges_created']} knowledge edges\")\n",
    "\n",
    "    def build_memorization_connections(self) -> None:\n",
    "        \"\"\"\n",
    "        Build memorization connections:\n",
    "        - Words to verses\n",
    "        - Verses to chapters\n",
    "        - Context windows for words\n",
    "        \"\"\"\n",
    "        edges_before = self.stats.get(\"edges_created\", 0)\n",
    "\n",
    "        # Process all chapters\n",
    "        for chapter_id in self.get_nodes_by_type(\"chapter\"):\n",
    "            chapter = self.quran[NIP.get_chapter_key(chapter_id)]\n",
    "\n",
    "            # Process verses\n",
    "            for verse in chapter.verses:\n",
    "                verse_id = NIG.for_verse(verse)\n",
    "\n",
    "                # Verse to chapter memorization edges\n",
    "                self.edge_manager.add_knowledge_edge(\n",
    "                    f\"{verse_id}:memorization\",\n",
    "                    f\"{chapter_id}:memorization\",\n",
    "                    Distribution.auto(weight=verse.get_letters_count())\n",
    "                )\n",
    "                self.stats[\"edges_created\"] += 1\n",
    "\n",
    "                # Word specific\n",
    "                word_nodes = []\n",
    "                for word in verse.words:\n",
    "                    if word.char_type_name == \"end\":  # Skip end markers\n",
    "                        continue\n",
    "\n",
    "                    word_id = NIG.for_word_instance(word, verse)\n",
    "                    word_nodes.append(word_id)\n",
    "\n",
    "                    # Word to verse memorization edges\n",
    "                    self.edge_manager.add_knowledge_edge(\n",
    "                        f\"{word_id}:memorization\",\n",
    "                        f\"{verse_id}:memorization\",\n",
    "                        Distribution.auto(weight=word.get_letters_count())\n",
    "                    )\n",
    "                    self.stats[\"edges_created\"] += 1\n",
    "\n",
    "                # Contextual memorization for words\n",
    "                if word_nodes:\n",
    "                    window_edges = self.edge_manager.add_gaussian_window_edges(\n",
    "                        [f\"{w}:memorization\" for w in word_nodes],\n",
    "                        window_size=min(3, len(word_nodes)),\n",
    "                        base_weight=0.5,\n",
    "                        std_scale=0.15\n",
    "                    )\n",
    "                    self.stats[\"edges_created\"] += window_edges\n",
    "\n",
    "        edges_created = self.stats[\"edges_created\"] - edges_before\n",
    "        logger.info(f\"Created {edges_created} memorization edges\")\n",
    "\n",
    "    def build_translation_connections(self) -> None:\n",
    "        \"\"\"\n",
    "        Build translation understanding connections:\n",
    "        - Words to verses\n",
    "        - Verses to chapters\n",
    "        - Word instances to word types\n",
    "        - Duplicate verse connections\n",
    "        \"\"\"\n",
    "        edges_before = self.stats.get(\"edges_created\", 0)\n",
    "\n",
    "        # Process chapters\n",
    "        for chapter_id in self.get_nodes_by_type(\"chapter\"):\n",
    "            chapter = self.quran[NIP.get_chapter_key(chapter_id)]\n",
    "\n",
    "            # Process verses\n",
    "            for verse in chapter.verses:\n",
    "                verse_id = NIG.for_verse(verse)\n",
    "\n",
    "                # Verse to chapter translation\n",
    "                self.edge_manager.add_knowledge_edge(\n",
    "                    f\"{verse_id}:translation\",\n",
    "                    f\"{chapter_id}:translation\",\n",
    "                    Distribution.auto(weight=verse.get_words_count())\n",
    "                )\n",
    "                self.stats[\"edges_created\"] += 1\n",
    "\n",
    "                # Process words\n",
    "                for word in verse.words:\n",
    "                    if word.char_type_name == \"end\":\n",
    "                        continue\n",
    "\n",
    "                    word_instance_id = NIG.for_word_instance(word, verse)\n",
    "                    word_type_id = NIG.for_word(word)\n",
    "\n",
    "                    # Word to verse translation\n",
    "                    self.edge_manager.add_knowledge_edge(\n",
    "                        f\"{word_instance_id}:translation\",\n",
    "                        f\"{verse_id}:translation\",\n",
    "                        Distribution.auto(weight=word.get_letters_count())\n",
    "                    )\n",
    "                    self.stats[\"edges_created\"] += 1\n",
    "\n",
    "                    # Word instance to word type translation\n",
    "                    self.edge_manager.add_knowledge_edge(\n",
    "                        f\"{word_instance_id}:translation\",\n",
    "                        f\"{word_type_id}:translation\",\n",
    "                        Distribution.normal(mean=0.9, std=0.1)\n",
    "                    )\n",
    "                    self.stats[\"edges_created\"] += 1\n",
    "\n",
    "        # Connect duplicate verses\n",
    "        duplicates = 0\n",
    "        for text, verse_keys in self.get_duplicated_verses():\n",
    "            # Connect all pairs\n",
    "            for i in range(len(verse_keys)):\n",
    "                for j in range(i+1, len(verse_keys)):\n",
    "                    v1, v2 = verse_keys[i], verse_keys[j]\n",
    "                    self.edge_manager.add_bidirectional_knowledge_edge(\n",
    "                        f\"{NIG.for_verse(v1)}:translation\",\n",
    "                        f\"{NIG.for_verse(v2)}:translation\",\n",
    "                        Distribution.normal(mean=0.9, std=0.1)\n",
    "                    )\n",
    "                    self.stats[\"edges_created\"] += 2\n",
    "                    duplicates += 1\n",
    "\n",
    "        logger.info(f\"Connected {duplicates} duplicate verse pairs\")\n",
    "\n",
    "        edges_created = self.stats[\"edges_created\"] - edges_before\n",
    "        logger.info(f\"Created {edges_created} translation edges\")\n",
    "\n",
    "    def build_grammar_connections(self) -> None:\n",
    "        \"\"\"\n",
    "        Build grammar connections:\n",
    "        - Words to lemmas\n",
    "        - Lemmas to roots\n",
    "        \"\"\"\n",
    "        edges_before = self.stats.get(\"edges_created\", 0)\n",
    "\n",
    "        # Process words\n",
    "        for word_id in self.get_nodes_by_type(\"word\"):\n",
    "            # Get lemmas\n",
    "            lemma_ids = self.node_manager.get_related_nodes(word_id, successor_type=\"lemma\")\n",
    "\n",
    "            for lemma_id in lemma_ids:\n",
    "                _, lemma = lemma_id.split(\":\", 1)\n",
    "\n",
    "                # Word to lemma translation (bidirectional)\n",
    "                self.edge_manager.add_bidirectional_knowledge_edge(\n",
    "                    f\"{word_id}:translation\",\n",
    "                    f\"{lemma_id}:translation\",\n",
    "                    Distribution.auto(weight=len(lemma))\n",
    "                )\n",
    "                self.stats[\"edges_created\"] += 2\n",
    "\n",
    "                # Get roots\n",
    "                root_ids = self.node_manager.get_related_nodes(lemma_id, successor_type=\"root\")\n",
    "\n",
    "                for root_id in root_ids:\n",
    "                    # Lemma to root meaning\n",
    "                    self.edge_manager.add_bidirectional_knowledge_edge(\n",
    "                        f\"{lemma_id}:translation\",\n",
    "                        f\"{root_id}:meaning\",\n",
    "                        Distribution.beta(alpha=4, beta=1.5)\n",
    "                    )\n",
    "                    self.stats[\"edges_created\"] += 2\n",
    "\n",
    "        edges_created = self.stats[\"edges_created\"] - edges_before\n",
    "        logger.info(f\"Created {edges_created} grammar edges\")\n",
    "\n",
    "    def connect_learning_dimensions(self) -> None:\n",
    "        \"\"\"\n",
    "        Connect different learning dimensions to model how they affect each other:\n",
    "        - Translation helps memorization\n",
    "        - Grammar helps translation\n",
    "        \"\"\"\n",
    "        edges_before = self.stats.get(\"edges_created\", 0)\n",
    "\n",
    "        # Translation helps memorization\n",
    "        logger.info(\"Connecting translation to memorization\")\n",
    "        for node_id in self.get_all_translatable_nodes():\n",
    "            self.edge_manager.add_knowledge_edge(\n",
    "                f\"{node_id}:translation\",\n",
    "                f\"{node_id}:memorization\",\n",
    "                Distribution.beta(alpha=3, beta=2)\n",
    "            )\n",
    "            self.stats[\"edges_created\"] += 1\n",
    "\n",
    "        # Grammar helps translation for words\n",
    "        logger.info(\"Connecting grammar to translation\")\n",
    "        for word_id in self.get_nodes_by_type(\"word\"):\n",
    "            self.edge_manager.add_knowledge_edge(\n",
    "                f\"{word_id}:grammar\",\n",
    "                f\"{word_id}:translation\",\n",
    "                Distribution.normal(mean=0.5, std=0.15)\n",
    "            )\n",
    "            self.stats[\"edges_created\"] += 1\n",
    "\n",
    "        edges_created = self.stats[\"edges_created\"] - edges_before\n",
    "        logger.info(f\"Created {edges_created} cross-dimension learning edges\")\n",
    "\n",
    "    def compile(self) -> None:\n",
    "        \"\"\"\n",
    "        Finalize the knowledge graph by computing all pending weights\n",
    "        and performing any necessary validations.\n",
    "        \"\"\"\n",
    "        if self._is_compiled:\n",
    "            raise RuntimeError(\"Knowledge graph has already been compiled\")\n",
    "\n",
    "        logger.info(\"Compiling knowledge graph\")\n",
    "\n",
    "        # Compile edge weights\n",
    "        self.edge_manager.compile()\n",
    "\n",
    "        # Validate final graph state\n",
    "        self._validate_compiled_graph()\n",
    "\n",
    "        # Update graph metadata\n",
    "        self.G.graph['knowledge_edges'] = self.stats[\"edges_created\"]\n",
    "        self.G.graph['knowledge_compiled'] = True\n",
    "\n",
    "        self._is_compiled = True\n",
    "        logger.info(\"Knowledge graph compiled successfully\")\n",
    "\n",
    "    def _validate_compiled_graph(self) -> None:\n",
    "        \"\"\"\n",
    "        Perform final validation checks on the compiled graph.\n",
    "        \"\"\"\n",
    "        # Ensure no edges are missing weights\n",
    "        for src, dst, data in self.G.edges(data=True):\n",
    "            if \"dist\" not in data:\n",
    "                is_exception = (\n",
    "                    data.get(\"type\") == \"dependency\"  # Dependency edges don't need weights\n",
    "                )\n",
    "                if not is_exception and data.get(\"knowledge_type\") is not None:\n",
    "                    raise ValueError(\n",
    "                        f\"Found edge missing weight distribution after compilation: \"\n",
    "                        f\"{src} -> {dst} {{{data}}}\"\n",
    "                    )\n",
    "\n",
    "    def save(self, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the compiled knowledge graph to a file.\n",
    "\n",
    "        Args:\n",
    "            filename: Path to save the graph\n",
    "        \"\"\"\n",
    "        if not self._is_compiled:\n",
    "            raise RuntimeError(\"Cannot save uncompiled knowledge graph. Call compile() first\")\n",
    "\n",
    "        logger.info(f\"Saving knowledge graph to {filename}\")\n",
    "\n",
    "        # Add timestamp\n",
    "        import datetime\n",
    "        self.G.graph['created_at'] = datetime.datetime.now().isoformat()\n",
    "        self.G.graph['node_count'] = self.G.number_of_nodes()\n",
    "        self.G.graph['edge_count'] = self.G.number_of_edges()\n",
    "\n",
    "        # Save graph\n",
    "        nx.write_graphml(self.G, filename)\n",
    "\n",
    "        # Save stats file\n",
    "        stats_file = Path(filename).with_suffix('.stats.json')\n",
    "        stats = {\n",
    "            'nodes': self.G.number_of_nodes(),\n",
    "            'edges': self.G.number_of_edges(),\n",
    "            'knowledge_edges': self.stats[\"edges_created\"],\n",
    "            'node_types': {\n",
    "                node_type: len(self.get_nodes_by_type(node_type))\n",
    "                for node_type in ['word', 'word_instance', 'verse', 'chapter', 'lemma', 'root']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "\n",
    "        logger.info(f\"Knowledge graph saved. Statistics written to {stats_file}\")\n",
    "\n",
    "\n",
    "\n",
    "def build_knowledge_graph(dependency_graph_path: str, output_path: str, quran: Quran) -> None:\n",
    "    \"\"\"\n",
    "    Build a knowledge graph from the dependency graph and Quran data.\n",
    "\n",
    "    Args:\n",
    "        dependency_graph_path: Path to the dependency graph (GraphML)\n",
    "        output_path: Path to save the resulting knowledge graph\n",
    "        quran: Quran data model\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading dependency graph from {dependency_graph_path}\")\n",
    "\n",
    "    # Load dependency graph\n",
    "    graph = nx.read_graphml(dependency_graph_path)\n",
    "\n",
    "    logger.info(f\"Loaded graph with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges\")\n",
    "\n",
    "    # Initialize builder\n",
    "    builder = KnowledgeGraphBuilder(graph, quran)\n",
    "\n",
    "    # Build knowledge connections\n",
    "    builder.build_all()\n",
    "\n",
    "    # Save the graph\n",
    "    builder.save(output_path)\n",
    "\n",
    "    logger.info(f\"Knowledge graph saved to {output_path}\")\n",
    "\n",
    "\n",
    "# graph = nx.read_graphml(\"quran_dependency_big.graphml\")\n",
    "# exp = KnowledgeExperiments(graph, quran)\n",
    "\n",
    "# # Try different configurations\n",
    "# exp.setup_standard_memorization()\n",
    "# exp.setup_tajweed_learning()\n",
    "# exp.setup_translation_understanding()\n",
    "# exp.setup_deep_understanding()\n",
    "# exp.setup_grammar_nodes()\n",
    "\n",
    "# # # Test experimental hypotheses\n",
    "# # exp.setup_experimental_learning()\n",
    "\n",
    "# # Save different configurations\n",
    "# # nx.write_graphml(graph, \"quran_knowledge_medium.graphml\")\n",
    "# # nx.write_graphml(graph, \"quran_knowledge_fatiha.graphml\")\n",
    "# exp.compile()\n",
    "# # exp.save(\"quran_knowledge_fatiha.graphml\")\n",
    "# # exp.save(\"quran_knowledge_medium.graphml\")\n",
    "# exp.save(\"quran_knowledge_big.graphml\")\n",
    "\n",
    "build_knowledge_graph(\"quran_dependency_big.graphml\", \"quran_knowledge_big.graphml\", quran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Build a knowledge graph for the Iqrah app\")\n",
    "    parser.add_argument(\"--input\", required=True, help=\"Path to dependency graph (GraphML)\")\n",
    "    parser.add_argument(\"--output\", required=True, help=\"Path to save knowledge graph\")\n",
    "    parser.add_argument(\"--quran-data\", required=True, help=\"Path to Quran data (pickle or JSON)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    try:\n",
    "        # Load Quran data\n",
    "        logger.info(f\"Loading Quran data from {args.quran_data}\")\n",
    "\n",
    "        quran_path = Path(args.quran_data)\n",
    "        if quran_path.suffix == '.pickle':\n",
    "            import pickle\n",
    "            with open(quran_path, 'rb') as f:\n",
    "                quran = pickle.load(f)\n",
    "        elif quran_path.suffix in ['.json', '.jsonl']:\n",
    "            # Implement JSON loading based on your Quran data format\n",
    "            raise NotImplementedError(\"JSON loading not implemented in this example\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported Quran data format: {quran_path.suffix}\")\n",
    "\n",
    "        # Build graph\n",
    "        build_knowledge_graph(args.input, args.output, quran)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building knowledge graph: {str(e)}\", exc_info=True)\n",
    "        sys.exit(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iqrah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}